server:
  api:
    port: 8000
agent:
  type: "LANGGRAPH"
  config:
    name: "Guardrails"
    graph_definition: "example_agent.py:app"
    observability:
      enabled: false
guardrails:
  enabled: true
  input:
    - type: custom_llm
      config:
        prompt: "hello"
        model_name: "gemini-2.5"
        message: "Cannot answer"
    - type: custom_llm
      config:
        prompt: "hello"
        model_name: "gemini-2.5"
  output:
    - type: guardrails_hub
      config:
        ban_words:
          - hello
          - bye
        message: "test"
    - type: custom_llm
      config:
        prompt: "hello"
        model_name: "gemini-2.5"
